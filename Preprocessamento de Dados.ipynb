{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Este notebook tem fun√ß√£o de limpar os datasets que foram extra√≠dos pela equipe Tera btc-dscbc_jan20 que teve por objetivo analisar alguns impactos do COVID-19 nos teores dos tweets no Brasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar bibliotecas para isso\n",
    "\n",
    "# bibliotecas gerais\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tqdm\n",
    "\n",
    "# bibliotecas para limpar texto\n",
    "import re\n",
    "import emoji\n",
    "import unicodedata\n",
    "from unicodedata import normalize\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "#biblioteca sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para retirar alfanum√©ricos e d√≠gitos\n",
    "\n",
    "def strip_characters(text):\n",
    "    t = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', text)\n",
    "    t = re.sub(r'\\(|\\)|:|,|;|\\.|‚Äô|‚Äù|‚Äú|\\?|%|>|<', '', t)\n",
    "    t = re.sub(r'/', ' ', t)\n",
    "    t = t.replace(r\"'\",'')\n",
    "    t = re.sub(r'(@\\w+)+', '', t)\n",
    "    t = t.lower()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o pasra remover caracteres especiais\n",
    "\n",
    "def remover_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para retirar emojis\n",
    "\n",
    "def strip_emoji(text):\n",
    "\n",
    "    new_text = re.sub(emoji.get_emoji_regexp(), r\"\", str(text))\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando stopwords em portugu√™s utilizando a biblioteca nltk\n",
    "import pt_core_news_sm\n",
    "nlp = pt_core_news_sm.load()\n",
    "pt_stopwords = sorted([token.text for token in nlp.vocab if token.is_stop])\n",
    "list_exclude = ['obrigado', 'bom', 'mal', 'nenhuma', 'maior',\n",
    "             'bem', 'n√£o', 'm√°ximo', 'boa', 'mais',\n",
    "               'bastante', 'certamente', 'certeza', 'contra',\n",
    "                'quarentena', 'coronav√≠rus', 'presidente', 'impeachment', 'demitido', 'demitida']\n",
    "for word in list_exclude:\n",
    "    nlp.vocab[word].is_stop = False\n",
    "list_include = set(['o', 'a', 't√°', 'ta', 'ser', 'pro', 'to', 't√¥', 'vc', 'voc√™', 'voce', 'pra',\n",
    "                    'pq', '√©', 'vou', 'que','t√£o', 'gt', 'de', 'da', 'do', 'em', 'uma', 'l√°',\n",
    "                    'j√°', 'no', 'para', 'na', 'com', 'um', 'minha', 'se', 'isso', 'por', 'vou',\n",
    "                    'os', 'isso', 'como', 'mesmo', 'tenho', 'aqui', 'ele', 'ela', 'quem', 'fazer',\n",
    "                    'eu', 's√≥', 'ai', 'mais', 's√≥', 'querer', 'https', 'ter', 'estar', 'ficar',\n",
    "                    'dos', 'das', 'vcs', 'tem', 'as', 'mas','ao'\n",
    "                    'tava', 'nao', 'sao', 'ja', 'so', 'nossa',\n",
    "                    'nosso', 'estao', 'tco', 'me', 'dia', 'te', 'ver', 'sera', 'porra', 'fez', 'ne',\n",
    "                    'kkk','kkkkkk', 'puta', 'kkkkkkkk', 'hj', 'afff', 'gbr', 'meu', 'cara', 'guri', 'cmg',\n",
    "                    'ctg', 'agr', 'pqp', 'vdd', 'eh', 'va', 'obg',\n",
    "                    'corona','virus','coronavirus','covid','covid19','19'\n",
    "                   'nem', 'numa', 'num', 'nuns', 'ces', 'voces', 'oce', 'oces', 'kkkk', 'vao', 'via',\n",
    "                    'hj', 'hoje', 'tudo', 'todo', 'toda',\n",
    "                    'vir', 'bem','ao','sem','ou','vai', 'dizer', 'entao', 'dizer', 'entao',\n",
    "                    'tao', 'tu', 'mim', 'mano', 'oq', 'pos', 'dm', 'dps',\n",
    "                    'coronavirusoutbreak', 'coronavirusPandemic', 'dar', 'vairus',\n",
    "                    'ainda', 'assim']\n",
    "                  )\n",
    "for w in list_include:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "stop_words = sorted([token.text for token in nlp.vocab if token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return list(set([word for word in words \n",
    "                     if len(word) > 1\n",
    "                     and not (word.isnumeric() and len(word) is not 4)\n",
    "                     and (not word.isnumeric() or word.isalpha())] )\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dos tokens\n",
    "\n",
    "df_tokens = pd.read_csv('tokens_unicos_10k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria√ß√£o de lista de tokens\n",
    "\n",
    "lista_token = []\n",
    "\n",
    "for row in range(0, len(df_tokens)):\n",
    "    lista_token.append(df_tokens.iloc[row,1])\n",
    "    \n",
    "lista_token = set(lista_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun√ß√£o para preprocessar palavras\n",
    "\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts1 = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts2 = [bigram_mod[doc] for doc in texts1]\n",
    "    texts3 = [trigram_mod[bigram_mod[doc]] for doc in texts2]\n",
    "    texts4 = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts3]    \n",
    "    return texts4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (1,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_03_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'Unnamed: 0', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('0304_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_02_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1245863582627033088</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>Bolsonaro Ao Vivo https://t.co/8NqTAHyVnA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divin√≥polis, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1245863582618619904</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>n√£o sei o q t√° acontecendo cmg nessa quarenten...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rio de Janeiro, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1245863581830144002</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>jro gra√ßas a esse fdp do corona eu vou estar i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>caralho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1245863581666570240</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>@veramagalhaes Vc est√° tranquila pq o D√≥ria j√°...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1245863579619741697</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>Saudade de aglomera√ß√µes, corona pf j√° deu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id           created_at  \\\n",
       "0           0  1245863582627033088  2020-04-02 23:59:59   \n",
       "1           1  1245863582618619904  2020-04-02 23:59:59   \n",
       "2           2  1245863581830144002  2020-04-02 23:59:59   \n",
       "3           3  1245863581666570240  2020-04-02 23:59:59   \n",
       "4           4  1245863579619741697  2020-04-02 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "0          Bolsonaro Ao Vivo https://t.co/8NqTAHyVnA          NaN   pt   \n",
       "1  n√£o sei o q t√° acontecendo cmg nessa quarenten...          NaN   pt   \n",
       "2  jro gra√ßas a esse fdp do corona eu vou estar i...          NaN   pt   \n",
       "3  @veramagalhaes Vc est√° tranquila pq o D√≥ria j√°...          NaN   pt   \n",
       "4          Saudade de aglomera√ß√µes, corona pf j√° deu          NaN   pt   \n",
       "\n",
       "   latitude  longitude                location  \n",
       "0       NaN        NaN     Divin√≥polis, Brasil  \n",
       "1       NaN        NaN  Rio de Janeiro, Brasil  \n",
       "2       NaN        NaN                 caralho  \n",
       "3       NaN        NaN                     NaN  \n",
       "4       NaN        NaN                  Brasil  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'Unnamed: 0', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245863582627033088</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>Bolsonaro Ao Vivo https://t.co/8NqTAHyVnA</td>\n",
       "      <td>bolsonaro ao vivo</td>\n",
       "      <td>bolsonaro ao vivo</td>\n",
       "      <td>bolsonaro ao vivo</td>\n",
       "      <td>[ao, bolsonaro, vivo]</td>\n",
       "      <td>['vivo', 'bolsonaro']</td>\n",
       "      <td>[vivo, bolsonaro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245863582618619904</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>n√£o sei o q t√° acontecendo cmg nessa quarenten...</td>\n",
       "      <td>n√£o sei o q t√° acontecendo cmg nessa quarenten...</td>\n",
       "      <td>nao sei o q ta acontecendo cmg nessa quarenten...</td>\n",
       "      <td>nao sei o q ta acontecendo cmg nessa quarenten...</td>\n",
       "      <td>[ja, ta, cmg, sei, acontecendo, nada, choro, q...</td>\n",
       "      <td>['cmg', 'nada', 'ja', 've', 'nao']</td>\n",
       "      <td>[nada, ve]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245863581830144002</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>jro gra√ßas a esse fdp do corona eu vou estar i...</td>\n",
       "      <td>jro gra√ßas a esse fdp do corona eu vou estar i...</td>\n",
       "      <td>jro gracas a esse fdp do corona eu vou estar i...</td>\n",
       "      <td>jro gracas a esse fdp do corona eu vou estar i...</td>\n",
       "      <td>[fdp, ir, mereco, sem, esse, corona, imenso, t...</td>\n",
       "      <td>['fdp', 'imenso', 'corona', 'tempo', 'aldeia',...</td>\n",
       "      <td>[fdp, imenso, tempo, aldeia, ir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245863581666570240</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>@veramagalhaes Vc est√° tranquila pq o D√≥ria j√°...</td>\n",
       "      <td>vc est√° tranquila pq o d√≥ria j√° lhe deu 500mi...</td>\n",
       "      <td>vc esta tranquila pq o doria ja lhe deu 500mi...</td>\n",
       "      <td>vc esta tranquila pq o doria ja lhe deu 500mi...</td>\n",
       "      <td>[politicos, em, 500mil, contrato, ja, de, chan...</td>\n",
       "      <td>['fake', 'jornalismo', 'chantagem', 'doria', '...</td>\n",
       "      <td>[fake, jornalismo, chantagem, doria, pandemia,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245863579619741697</td>\n",
       "      <td>2020-04-02 23:59:59</td>\n",
       "      <td>Saudade de aglomera√ß√µes, corona pf j√° deu</td>\n",
       "      <td>saudade de aglomera√ß√µes corona pf j√° deu</td>\n",
       "      <td>saudade de aglomeracoes corona pf ja deu</td>\n",
       "      <td>saudade de aglomeracoes corona pf ja deu</td>\n",
       "      <td>[aglomeracoes, pf, saudade, corona, deu, ja, de]</td>\n",
       "      <td>['pf', 'saudade', 'corona', 'ja']</td>\n",
       "      <td>[pf, saudade]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1245863582627033088  2020-04-02 23:59:59   \n",
       "1  1245863582618619904  2020-04-02 23:59:59   \n",
       "2  1245863581830144002  2020-04-02 23:59:59   \n",
       "3  1245863581666570240  2020-04-02 23:59:59   \n",
       "4  1245863579619741697  2020-04-02 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "0          Bolsonaro Ao Vivo https://t.co/8NqTAHyVnA   \n",
       "1  n√£o sei o q t√° acontecendo cmg nessa quarenten...   \n",
       "2  jro gra√ßas a esse fdp do corona eu vou estar i...   \n",
       "3  @veramagalhaes Vc est√° tranquila pq o D√≥ria j√°...   \n",
       "4          Saudade de aglomera√ß√µes, corona pf j√° deu   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0                                 bolsonaro ao vivo    \n",
       "1  n√£o sei o q t√° acontecendo cmg nessa quarenten...   \n",
       "2  jro gra√ßas a esse fdp do corona eu vou estar i...   \n",
       "3   vc est√° tranquila pq o d√≥ria j√° lhe deu 500mi...   \n",
       "4           saudade de aglomera√ß√µes corona pf j√° deu   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0                                 bolsonaro ao vivo    \n",
       "1  nao sei o q ta acontecendo cmg nessa quarenten...   \n",
       "2  jro gracas a esse fdp do corona eu vou estar i...   \n",
       "3   vc esta tranquila pq o doria ja lhe deu 500mi...   \n",
       "4           saudade de aglomeracoes corona pf ja deu   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0                                 bolsonaro ao vivo    \n",
       "1  nao sei o q ta acontecendo cmg nessa quarenten...   \n",
       "2  jro gracas a esse fdp do corona eu vou estar i...   \n",
       "3   vc esta tranquila pq o doria ja lhe deu 500mi...   \n",
       "4           saudade de aglomeracoes corona pf ja deu   \n",
       "\n",
       "                                               token  \\\n",
       "0                              [ao, bolsonaro, vivo]   \n",
       "1  [ja, ta, cmg, sei, acontecendo, nada, choro, q...   \n",
       "2  [fdp, ir, mereco, sem, esse, corona, imenso, t...   \n",
       "3  [politicos, em, 500mil, contrato, ja, de, chan...   \n",
       "4   [aglomeracoes, pf, saudade, corona, deu, ja, de]   \n",
       "\n",
       "                                              token2  \\\n",
       "0                              ['vivo', 'bolsonaro']   \n",
       "1                 ['cmg', 'nada', 'ja', 've', 'nao']   \n",
       "2  ['fdp', 'imenso', 'corona', 'tempo', 'aldeia',...   \n",
       "3  ['fake', 'jornalismo', 'chantagem', 'doria', '...   \n",
       "4                  ['pf', 'saudade', 'corona', 'ja']   \n",
       "\n",
       "                                              token3  \n",
       "0                                  [vivo, bolsonaro]  \n",
       "1                                         [nada, ve]  \n",
       "2                   [fdp, imenso, tempo, aldeia, ir]  \n",
       "3  [fake, jornalismo, chantagem, doria, pandemia,...  \n",
       "4                                      [pf, saudade]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('0204_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_01_04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245501195344646145</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>em tempos de pandemia.. o mundo um caos.. e so...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recife, Pernambuco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245501195227213825</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>@LuisH_fad @DanPerdigaoo @DiiegoDigs  foi muit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245501193910194177</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>Eu s√≥ queria que o Cine Bangue colocasse Yo√±lu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cabedelo, Para√≠ba, Brasil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245501193431982081</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>@CarlosBolsonaro Obrigado, fam√≠lia Bolsonaro p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campo Alegre, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245501193230725121</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>ler quarto de despejo nessa quarentena t√° send...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1245501195344646145  2020-04-01 23:59:59   \n",
       "1  1245501195227213825  2020-04-01 23:59:59   \n",
       "2  1245501193910194177  2020-04-01 23:59:59   \n",
       "3  1245501193431982081  2020-04-01 23:59:59   \n",
       "4  1245501193230725121  2020-04-01 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "0  em tempos de pandemia.. o mundo um caos.. e so...          NaN   pt   \n",
       "1  @LuisH_fad @DanPerdigaoo @DiiegoDigs  foi muit...          NaN   pt   \n",
       "2  Eu s√≥ queria que o Cine Bangue colocasse Yo√±lu...          NaN   pt   \n",
       "3  @CarlosBolsonaro Obrigado, fam√≠lia Bolsonaro p...          NaN   pt   \n",
       "4  ler quarto de despejo nessa quarentena t√° send...          NaN   pt   \n",
       "\n",
       "   latitude  longitude                    location  \n",
       "0       NaN        NaN          Recife, Pernambuco  \n",
       "1       NaN        NaN                         NaN  \n",
       "2       NaN        NaN  Cabedelo, Para√≠ba, Brasil.  \n",
       "3       NaN        NaN        Campo Alegre, Brasil  \n",
       "4       NaN        NaN                      Brasil  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245501195344646145</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>em tempos de pandemia.. o mundo um caos.. e so...</td>\n",
       "      <td>em tempos de pandemia o mundo um caos e so pen...</td>\n",
       "      <td>em tempos de pandemia o mundo um caos e so pen...</td>\n",
       "      <td>em tempos de pandemia o mundo um caos e so pen...</td>\n",
       "      <td>[ta, em, deve, tempos, povo, fotos, por, de, l...</td>\n",
       "      <td>['lavar', 'pandemia', 'ter', 'povo', 'so', 'ma...</td>\n",
       "      <td>[lavar, pandemia, povo, mao, mundo, luana, pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245501195227213825</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>@LuisH_fad @DanPerdigaoo @DiiegoDigs  foi muit...</td>\n",
       "      <td>foi muito bom conhecer voces mas dessa qua...</td>\n",
       "      <td>foi muito bom conhecer voces mas dessa qua...</td>\n",
       "      <td>foi muito bom conhecer voces mas dessa qua...</td>\n",
       "      <td>[mas, bom, voces, muito, conhecer, passo, eu, ...</td>\n",
       "      <td>['bom', 'voces', 'muito', 'conhecer', 'passo',...</td>\n",
       "      <td>[bom, muito, conhecer, passo, adeus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245501193910194177</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>Eu s√≥ queria que o Cine Bangue colocasse Yo√±lu...</td>\n",
       "      <td>eu s√≥ queria que o cine bangue colocasse yo√±lu...</td>\n",
       "      <td>eu so queria que o cine bangue colocasse yonlu...</td>\n",
       "      <td>eu so queria que o cine bangue colocasse yonlu...</td>\n",
       "      <td>[cine, em, dessa, de, eu, queria, muito, pra, ...</td>\n",
       "      <td>['filme', 'pandemia', 'muito', 'me', 'acabar',...</td>\n",
       "      <td>[filme, pandemia, muito, acabar, chorar, depoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245501193431982081</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>@CarlosBolsonaro Obrigado, fam√≠lia Bolsonaro p...</td>\n",
       "      <td>obrigado fam√≠lia bolsonaro por implantar o so...</td>\n",
       "      <td>obrigado familia bolsonaro por implantar o so...</td>\n",
       "      <td>obrigado familia bolsonaro por implantar o so...</td>\n",
       "      <td>[hein, socialismo, brasil, implantar, que, bol...</td>\n",
       "      <td>['hein', 'socialismo', 'brasil', 'implantar', ...</td>\n",
       "      <td>[hein, socialismo, brasil, implantar, bolsonar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245501193230725121</td>\n",
       "      <td>2020-04-01 23:59:59</td>\n",
       "      <td>ler quarto de despejo nessa quarentena t√° send...</td>\n",
       "      <td>ler quarto de despejo nessa quarentena t√° send...</td>\n",
       "      <td>ler quarto de despejo nessa quarentena ta send...</td>\n",
       "      <td>ler quarto de despejo nessa quarentena ta send...</td>\n",
       "      <td>[sendo, na, minha, um, ta, ler, pra, quarto, a...</td>\n",
       "      <td>['ler', 'quarto', 'acordar']</td>\n",
       "      <td>[ler, quarto, acordar]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1245501195344646145  2020-04-01 23:59:59   \n",
       "1  1245501195227213825  2020-04-01 23:59:59   \n",
       "2  1245501193910194177  2020-04-01 23:59:59   \n",
       "3  1245501193431982081  2020-04-01 23:59:59   \n",
       "4  1245501193230725121  2020-04-01 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  em tempos de pandemia.. o mundo um caos.. e so...   \n",
       "1  @LuisH_fad @DanPerdigaoo @DiiegoDigs  foi muit...   \n",
       "2  Eu s√≥ queria que o Cine Bangue colocasse Yo√±lu...   \n",
       "3  @CarlosBolsonaro Obrigado, fam√≠lia Bolsonaro p...   \n",
       "4  ler quarto de despejo nessa quarentena t√° send...   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0  em tempos de pandemia o mundo um caos e so pen...   \n",
       "1      foi muito bom conhecer voces mas dessa qua...   \n",
       "2  eu s√≥ queria que o cine bangue colocasse yo√±lu...   \n",
       "3   obrigado fam√≠lia bolsonaro por implantar o so...   \n",
       "4  ler quarto de despejo nessa quarentena t√° send...   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0  em tempos de pandemia o mundo um caos e so pen...   \n",
       "1      foi muito bom conhecer voces mas dessa qua...   \n",
       "2  eu so queria que o cine bangue colocasse yonlu...   \n",
       "3   obrigado familia bolsonaro por implantar o so...   \n",
       "4  ler quarto de despejo nessa quarentena ta send...   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0  em tempos de pandemia o mundo um caos e so pen...   \n",
       "1      foi muito bom conhecer voces mas dessa qua...   \n",
       "2  eu so queria que o cine bangue colocasse yonlu...   \n",
       "3   obrigado familia bolsonaro por implantar o so...   \n",
       "4  ler quarto de despejo nessa quarentena ta send...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [ta, em, deve, tempos, povo, fotos, por, de, l...   \n",
       "1  [mas, bom, voces, muito, conhecer, passo, eu, ...   \n",
       "2  [cine, em, dessa, de, eu, queria, muito, pra, ...   \n",
       "3  [hein, socialismo, brasil, implantar, que, bol...   \n",
       "4  [sendo, na, minha, um, ta, ler, pra, quarto, a...   \n",
       "\n",
       "                                              token2  \\\n",
       "0  ['lavar', 'pandemia', 'ter', 'povo', 'so', 'ma...   \n",
       "1  ['bom', 'voces', 'muito', 'conhecer', 'passo',...   \n",
       "2  ['filme', 'pandemia', 'muito', 'me', 'acabar',...   \n",
       "3  ['hein', 'socialismo', 'brasil', 'implantar', ...   \n",
       "4                       ['ler', 'quarto', 'acordar']   \n",
       "\n",
       "                                              token3  \n",
       "0  [lavar, pandemia, povo, mao, mundo, luana, pos...  \n",
       "1               [bom, muito, conhecer, passo, adeus]  \n",
       "2  [filme, pandemia, muito, acabar, chorar, depoi...  \n",
       "3  [hein, socialismo, brasil, implantar, bolsonar...  \n",
       "4                             [ler, quarto, acordar]  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('0104_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_31_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245138807332253696</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>@LeilaneNeubarth Gostei da fala do presidente ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bel√©m, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245138807151878145</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>In√≠cio da quarentena // Metade da quarentena h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245138806984134656</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>eu olho pro bolsonaro e vejo o reflexo do hitl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>derry üéà</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245138806891843585</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>@zitosilva Eu sinto um misto: quero que saia /...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Recife, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245138806543732740</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>#bbb20 Bolsonaro trocou a boca pelo c√∫ em rede...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1245138807332253696  2020-03-31 23:59:59   \n",
       "1  1245138807151878145  2020-03-31 23:59:59   \n",
       "2  1245138806984134656  2020-03-31 23:59:59   \n",
       "3  1245138806891843585  2020-03-31 23:59:59   \n",
       "4  1245138806543732740  2020-03-31 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "0  @LeilaneNeubarth Gostei da fala do presidente ...          NaN   pt   \n",
       "1  In√≠cio da quarentena // Metade da quarentena h...          NaN   pt   \n",
       "2  eu olho pro bolsonaro e vejo o reflexo do hitl...          NaN   pt   \n",
       "3  @zitosilva Eu sinto um misto: quero que saia /...          NaN   pt   \n",
       "4  #bbb20 Bolsonaro trocou a boca pelo c√∫ em rede...          NaN   pt   \n",
       "\n",
       "   latitude  longitude        location  \n",
       "0       NaN        NaN   Bel√©m, Brasil  \n",
       "1       NaN        NaN             NaN  \n",
       "2       NaN        NaN         derry üéà  \n",
       "3       NaN        NaN  Recife, Brasil  \n",
       "4       NaN        NaN             NaN  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1245138807332253696</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>@LeilaneNeubarth Gostei da fala do presidente ...</td>\n",
       "      <td>gostei da fala do presidente  \\r\\nagradeceu a...</td>\n",
       "      <td>gostei da fala do presidente  \\r\\nagradeceu a...</td>\n",
       "      <td>gostei da fala do presidente  \\r\\nagradeceu a...</td>\n",
       "      <td>[imprensa, gostei, agradeceu, menos, da, do, p...</td>\n",
       "      <td>['menos', 'presidente']</td>\n",
       "      <td>[menos, presidente]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245138807151878145</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>In√≠cio da quarentena // Metade da quarentena h...</td>\n",
       "      <td>in√≠cio da quarentena    metade da quarentena</td>\n",
       "      <td>inicio da quarentena    metade da quarentena</td>\n",
       "      <td>inicio da quarentena    metade da quarentena</td>\n",
       "      <td>[da, inicio, metade, quarentena]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1245138806984134656</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>eu olho pro bolsonaro e vejo o reflexo do hitl...</td>\n",
       "      <td>eu olho pro bolsonaro e vejo o reflexo do hitl...</td>\n",
       "      <td>eu olho pro bolsonaro e vejo o reflexo do hitl...</td>\n",
       "      <td>eu olho pro bolsonaro e vejo o reflexo do hitl...</td>\n",
       "      <td>[medo, quero, vejo, odio, hitler, olho, reflex...</td>\n",
       "      <td>['medo', 'odio', 'hitler', 'olho', 'reflexo', ...</td>\n",
       "      <td>[medo, odio, hitler, olho, reflexo, chorar, bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245138806891843585</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>@zitosilva Eu sinto um misto: quero que saia /...</td>\n",
       "      <td>eu sinto um misto quero que saia   tenho medo...</td>\n",
       "      <td>eu sinto um misto quero que saia   tenho medo...</td>\n",
       "      <td>eu sinto um misto quero que saia   tenho medo...</td>\n",
       "      <td>[misto, medo, quero, assume, bem, tenho, de, e...</td>\n",
       "      <td>['misto', 'medo', 'diferente', 'bem', 'mourao'...</td>\n",
       "      <td>[misto, medo, diferente, mourao, bolsonaro, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245138806543732740</td>\n",
       "      <td>2020-03-31 23:59:59</td>\n",
       "      <td>#bbb20 Bolsonaro trocou a boca pelo c√∫ em rede...</td>\n",
       "      <td>#bbb20 bolsonaro trocou a boca pelo c√∫ em rede...</td>\n",
       "      <td>#bbb20 bolsonaro trocou a boca pelo cu em rede...</td>\n",
       "      <td>#bbb20 bolsonaro trocou a boca pelo cu em rede...</td>\n",
       "      <td>[trocou, rede, nacional, em, bbb20, pelo, bols...</td>\n",
       "      <td>['rede', 'nacional', 'bolsonaro', 'boca', 'cu']</td>\n",
       "      <td>[rede, nacional, bolsonaro, boca, cu]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1245138807332253696  2020-03-31 23:59:59   \n",
       "1  1245138807151878145  2020-03-31 23:59:59   \n",
       "2  1245138806984134656  2020-03-31 23:59:59   \n",
       "3  1245138806891843585  2020-03-31 23:59:59   \n",
       "4  1245138806543732740  2020-03-31 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  @LeilaneNeubarth Gostei da fala do presidente ...   \n",
       "1  In√≠cio da quarentena // Metade da quarentena h...   \n",
       "2  eu olho pro bolsonaro e vejo o reflexo do hitl...   \n",
       "3  @zitosilva Eu sinto um misto: quero que saia /...   \n",
       "4  #bbb20 Bolsonaro trocou a boca pelo c√∫ em rede...   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0   gostei da fala do presidente  \\r\\nagradeceu a...   \n",
       "1      in√≠cio da quarentena    metade da quarentena    \n",
       "2  eu olho pro bolsonaro e vejo o reflexo do hitl...   \n",
       "3   eu sinto um misto quero que saia   tenho medo...   \n",
       "4  #bbb20 bolsonaro trocou a boca pelo c√∫ em rede...   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0   gostei da fala do presidente  \\r\\nagradeceu a...   \n",
       "1      inicio da quarentena    metade da quarentena    \n",
       "2  eu olho pro bolsonaro e vejo o reflexo do hitl...   \n",
       "3   eu sinto um misto quero que saia   tenho medo...   \n",
       "4  #bbb20 bolsonaro trocou a boca pelo cu em rede...   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0   gostei da fala do presidente  \\r\\nagradeceu a...   \n",
       "1      inicio da quarentena    metade da quarentena    \n",
       "2  eu olho pro bolsonaro e vejo o reflexo do hitl...   \n",
       "3   eu sinto um misto quero que saia   tenho medo...   \n",
       "4  #bbb20 bolsonaro trocou a boca pelo cu em rede...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [imprensa, gostei, agradeceu, menos, da, do, p...   \n",
       "1                   [da, inicio, metade, quarentena]   \n",
       "2  [medo, quero, vejo, odio, hitler, olho, reflex...   \n",
       "3  [misto, medo, quero, assume, bem, tenho, de, e...   \n",
       "4  [trocou, rede, nacional, em, bbb20, pelo, bols...   \n",
       "\n",
       "                                              token2  \\\n",
       "0                            ['menos', 'presidente']   \n",
       "1                                                 []   \n",
       "2  ['medo', 'odio', 'hitler', 'olho', 'reflexo', ...   \n",
       "3  ['misto', 'medo', 'diferente', 'bem', 'mourao'...   \n",
       "4    ['rede', 'nacional', 'bolsonaro', 'boca', 'cu']   \n",
       "\n",
       "                                              token3  \n",
       "0                                [menos, presidente]  \n",
       "1                                                 []  \n",
       "2  [medo, odio, hitler, olho, reflexo, chorar, bo...  \n",
       "3  [misto, medo, diferente, mourao, bolsonaro, in...  \n",
       "4              [rede, nacional, bolsonaro, boca, cu]  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('3103_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_29.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244414031575887880</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Desespero nos EUA. @realDonaldTrump chegou a o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S√£o Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244414030900596736</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@HaddadDebochado ABESTADOHHH?????? O PRESIDENT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maca√©, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244414030342750208</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contagem, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244414030070140928</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@RASPUTINDECARV1 Nand√£o curtindo que o bolsona...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244414029998759938</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@delbura Queria, o tema vai ser corona v√≠rus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1244414031575887880  2020-03-29 23:59:59   \n",
       "1  1244414030900596736  2020-03-29 23:59:59   \n",
       "2  1244414030342750208  2020-03-29 23:59:59   \n",
       "3  1244414030070140928  2020-03-29 23:59:59   \n",
       "4  1244414029998759938  2020-03-29 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "0  Desespero nos EUA. @realDonaldTrump chegou a o...          NaN   pt   \n",
       "1  @HaddadDebochado ABESTADOHHH?????? O PRESIDENT...          NaN   pt   \n",
       "2  Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...          NaN   pt   \n",
       "3  @RASPUTINDECARV1 Nand√£o curtindo que o bolsona...          NaN   pt   \n",
       "4       @delbura Queria, o tema vai ser corona v√≠rus          NaN   pt   \n",
       "\n",
       "   latitude  longitude           location  \n",
       "0       NaN        NaN  S√£o Paulo, Brazil  \n",
       "1       NaN        NaN      Maca√©, Brasil  \n",
       "2       NaN        NaN   Contagem, Brasil  \n",
       "3       NaN        NaN                NaN  \n",
       "4       NaN        NaN                NaN  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244414031575887880</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Desespero nos EUA. @realDonaldTrump chegou a o...</td>\n",
       "      <td>desespero nos eua  chegou a obrigar por lei qu...</td>\n",
       "      <td>desespero nos eua  chegou a obrigar por lei qu...</td>\n",
       "      <td>desespero nos eua  chegou a obrigar por lei qu...</td>\n",
       "      <td>[serao, lei, panico, amigos, por, de, respirad...</td>\n",
       "      <td>['serao', 'lei', 'panico', 'eua', 'obrigar', '...</td>\n",
       "      <td>[serao, lei, panico, eua, obrigar, fabricar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244414030900596736</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@HaddadDebochado ABESTADOHHH?????? O PRESIDENT...</td>\n",
       "      <td>abestadohhh o presidente realmente estava doente</td>\n",
       "      <td>abestadohhh o presidente realmente estava doente</td>\n",
       "      <td>abestadohhh o presidente realmente estava doente</td>\n",
       "      <td>[abestadohhh, realmente, presidente, doente, e...</td>\n",
       "      <td>['realmente', 'doente', 'presidente']</td>\n",
       "      <td>[realmente, doente, presidente]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244414030342750208</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...</td>\n",
       "      <td>secret√°rio da sa√∫de do cear√° dr cabeto testa p...</td>\n",
       "      <td>secretario da saude do ceara dr cabeto testa p...</td>\n",
       "      <td>secretario da saude do ceara dr cabeto testa p...</td>\n",
       "      <td>[testa, saude, novo, coronavirus, cabeto, posi...</td>\n",
       "      <td>['saude', 'novo', 'coronavirus', 'cabeto', 'po...</td>\n",
       "      <td>[saude, novo, cabeto, positivo, dr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244414030070140928</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@RASPUTINDECARV1 Nand√£o curtindo que o bolsona...</td>\n",
       "      <td>nand√£o curtindo que o bolsonarismo est√° viran...</td>\n",
       "      <td>nandao curtindo que o bolsonarismo esta viran...</td>\n",
       "      <td>nandao curtindo que o bolsonarismo esta viran...</td>\n",
       "      <td>[minhas, isso, nandotenho, curtindo, falou, na...</td>\n",
       "      <td>['bolsonarismo', 'me', 'besteira']</td>\n",
       "      <td>[bolsonarismo, besteira]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244414029998759938</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@delbura Queria, o tema vai ser corona v√≠rus</td>\n",
       "      <td>queria o tema vai ser corona v√≠rus</td>\n",
       "      <td>queria o tema vai ser corona virus</td>\n",
       "      <td>queria o tema vai ser corona virus</td>\n",
       "      <td>[queria, tema, vai, corona, virus, ser]</td>\n",
       "      <td>['corona', 'virus']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1244414031575887880  2020-03-29 23:59:59   \n",
       "1  1244414030900596736  2020-03-29 23:59:59   \n",
       "2  1244414030342750208  2020-03-29 23:59:59   \n",
       "3  1244414030070140928  2020-03-29 23:59:59   \n",
       "4  1244414029998759938  2020-03-29 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Desespero nos EUA. @realDonaldTrump chegou a o...   \n",
       "1  @HaddadDebochado ABESTADOHHH?????? O PRESIDENT...   \n",
       "2  Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...   \n",
       "3  @RASPUTINDECARV1 Nand√£o curtindo que o bolsona...   \n",
       "4       @delbura Queria, o tema vai ser corona v√≠rus   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0  desespero nos eua  chegou a obrigar por lei qu...   \n",
       "1   abestadohhh o presidente realmente estava doente   \n",
       "2  secret√°rio da sa√∫de do cear√° dr cabeto testa p...   \n",
       "3   nand√£o curtindo que o bolsonarismo est√° viran...   \n",
       "4                 queria o tema vai ser corona v√≠rus   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0  desespero nos eua  chegou a obrigar por lei qu...   \n",
       "1   abestadohhh o presidente realmente estava doente   \n",
       "2  secretario da saude do ceara dr cabeto testa p...   \n",
       "3   nandao curtindo que o bolsonarismo esta viran...   \n",
       "4                 queria o tema vai ser corona virus   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0  desespero nos eua  chegou a obrigar por lei qu...   \n",
       "1   abestadohhh o presidente realmente estava doente   \n",
       "2  secretario da saude do ceara dr cabeto testa p...   \n",
       "3   nandao curtindo que o bolsonarismo esta viran...   \n",
       "4                 queria o tema vai ser corona virus   \n",
       "\n",
       "                                               token  \\\n",
       "0  [serao, lei, panico, amigos, por, de, respirad...   \n",
       "1  [abestadohhh, realmente, presidente, doente, e...   \n",
       "2  [testa, saude, novo, coronavirus, cabeto, posi...   \n",
       "3  [minhas, isso, nandotenho, curtindo, falou, na...   \n",
       "4            [queria, tema, vai, corona, virus, ser]   \n",
       "\n",
       "                                              token2  \\\n",
       "0  ['serao', 'lei', 'panico', 'eua', 'obrigar', '...   \n",
       "1              ['realmente', 'doente', 'presidente']   \n",
       "2  ['saude', 'novo', 'coronavirus', 'cabeto', 'po...   \n",
       "3                 ['bolsonarismo', 'me', 'besteira']   \n",
       "4                                ['corona', 'virus']   \n",
       "\n",
       "                                         token3  \n",
       "0  [serao, lei, panico, eua, obrigar, fabricar]  \n",
       "1               [realmente, doente, presidente]  \n",
       "2           [saude, novo, cabeto, positivo, dr]  \n",
       "3                      [bolsonarismo, besteira]  \n",
       "4                                            []  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('2903_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_28csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>created_at</td>\n",
       "      <td>full_text</td>\n",
       "      <td>country_code</td>\n",
       "      <td>lang</td>\n",
       "      <td>latitude</td>\n",
       "      <td>longitude</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244414031575887880</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Desespero nos EUA. @realDonaldTrump chegou a o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S√£o Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244414030900596736</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@HaddadDebochado ABESTADOHHH?????? O PRESIDENT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maca√©, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244414030342750208</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contagem, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244414030070140928</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@RASPUTINDECARV1 Nand√£o curtindo que o bolsona...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0                   id           created_at   \n",
       "1  1244414031575887880  2020-03-29 23:59:59   \n",
       "2  1244414030900596736  2020-03-29 23:59:59   \n",
       "3  1244414030342750208  2020-03-29 23:59:59   \n",
       "4  1244414030070140928  2020-03-29 23:59:59   \n",
       "\n",
       "                                           full_text  country_code  lang  \\\n",
       "0                                          full_text  country_code  lang   \n",
       "1  Desespero nos EUA. @realDonaldTrump chegou a o...           NaN    pt   \n",
       "2  @HaddadDebochado ABESTADOHHH?????? O PRESIDENT...           NaN    pt   \n",
       "3  Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...           NaN    pt   \n",
       "4  @RASPUTINDECARV1 Nand√£o curtindo que o bolsona...           NaN    pt   \n",
       "\n",
       "   latitude  longitude           location  \n",
       "0  latitude  longitude           location  \n",
       "1       NaN        NaN  S√£o Paulo, Brazil  \n",
       "2       NaN        NaN      Maca√©, Brasil  \n",
       "3       NaN        NaN   Contagem, Brasil  \n",
       "4       NaN        NaN                NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244414031575887880</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Desespero nos EUA. @realDonaldTrump chegou a o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S√£o Paulo, Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244414030900596736</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@HaddadDebochado ABESTADOHHH?????? O PRESIDENT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maca√©, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244414030342750208</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Contagem, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244414030070140928</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@RASPUTINDECARV1 Nand√£o curtindo que o bolsona...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1244414029998759938</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@delbura Queria, o tema vai ser corona v√≠rus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "1  1244414031575887880  2020-03-29 23:59:59   \n",
       "2  1244414030900596736  2020-03-29 23:59:59   \n",
       "3  1244414030342750208  2020-03-29 23:59:59   \n",
       "4  1244414030070140928  2020-03-29 23:59:59   \n",
       "5  1244414029998759938  2020-03-29 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "1  Desespero nos EUA. @realDonaldTrump chegou a o...          NaN   pt   \n",
       "2  @HaddadDebochado ABESTADOHHH?????? O PRESIDENT...          NaN   pt   \n",
       "3  Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...          NaN   pt   \n",
       "4  @RASPUTINDECARV1 Nand√£o curtindo que o bolsona...          NaN   pt   \n",
       "5       @delbura Queria, o tema vai ser corona v√≠rus          NaN   pt   \n",
       "\n",
       "  latitude longitude           location  \n",
       "1      NaN       NaN  S√£o Paulo, Brazil  \n",
       "2      NaN       NaN      Maca√©, Brasil  \n",
       "3      NaN       NaN   Contagem, Brasil  \n",
       "4      NaN       NaN                NaN  \n",
       "5      NaN       NaN                NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1244414031575887880</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Desespero nos EUA. @realDonaldTrump chegou a o...</td>\n",
       "      <td>desespero nos eua  chegou a obrigar por lei qu...</td>\n",
       "      <td>desespero nos eua  chegou a obrigar por lei qu...</td>\n",
       "      <td>desespero nos eua  chegou a obrigar por lei qu...</td>\n",
       "      <td>[serao, lei, panico, amigos, por, de, respirad...</td>\n",
       "      <td>['serao', 'lei', 'panico', 'eua', 'obrigar', '...</td>\n",
       "      <td>[serao, lei, panico, eua, obrigar, fabricar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1244414030900596736</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@HaddadDebochado ABESTADOHHH?????? O PRESIDENT...</td>\n",
       "      <td>abestadohhh o presidente realmente estava doente</td>\n",
       "      <td>abestadohhh o presidente realmente estava doente</td>\n",
       "      <td>abestadohhh o presidente realmente estava doente</td>\n",
       "      <td>[abestadohhh, realmente, presidente, doente, e...</td>\n",
       "      <td>['realmente', 'doente', 'presidente']</td>\n",
       "      <td>[realmente, doente, presidente]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1244414030342750208</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...</td>\n",
       "      <td>secret√°rio da sa√∫de do cear√° dr cabeto testa p...</td>\n",
       "      <td>secretario da saude do ceara dr cabeto testa p...</td>\n",
       "      <td>secretario da saude do ceara dr cabeto testa p...</td>\n",
       "      <td>[testa, saude, novo, coronavirus, cabeto, posi...</td>\n",
       "      <td>['saude', 'novo', 'coronavirus', 'cabeto', 'po...</td>\n",
       "      <td>[saude, novo, cabeto, positivo, dr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1244414030070140928</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@RASPUTINDECARV1 Nand√£o curtindo que o bolsona...</td>\n",
       "      <td>nand√£o curtindo que o bolsonarismo est√° viran...</td>\n",
       "      <td>nandao curtindo que o bolsonarismo esta viran...</td>\n",
       "      <td>nandao curtindo que o bolsonarismo esta viran...</td>\n",
       "      <td>[minhas, isso, nandotenho, curtindo, falou, na...</td>\n",
       "      <td>['bolsonarismo', 'me', 'besteira']</td>\n",
       "      <td>[bolsonarismo, besteira]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1244414029998759938</td>\n",
       "      <td>2020-03-29 23:59:59</td>\n",
       "      <td>@delbura Queria, o tema vai ser corona v√≠rus</td>\n",
       "      <td>queria o tema vai ser corona v√≠rus</td>\n",
       "      <td>queria o tema vai ser corona virus</td>\n",
       "      <td>queria o tema vai ser corona virus</td>\n",
       "      <td>[queria, tema, vai, corona, virus, ser]</td>\n",
       "      <td>['corona', 'virus']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "1  1244414031575887880  2020-03-29 23:59:59   \n",
       "2  1244414030900596736  2020-03-29 23:59:59   \n",
       "3  1244414030342750208  2020-03-29 23:59:59   \n",
       "4  1244414030070140928  2020-03-29 23:59:59   \n",
       "5  1244414029998759938  2020-03-29 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "1  Desespero nos EUA. @realDonaldTrump chegou a o...   \n",
       "2  @HaddadDebochado ABESTADOHHH?????? O PRESIDENT...   \n",
       "3  Secret√°rio da Sa√∫de do Cear√°, Dr. Cabeto, test...   \n",
       "4  @RASPUTINDECARV1 Nand√£o curtindo que o bolsona...   \n",
       "5       @delbura Queria, o tema vai ser corona v√≠rus   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "1  desespero nos eua  chegou a obrigar por lei qu...   \n",
       "2   abestadohhh o presidente realmente estava doente   \n",
       "3  secret√°rio da sa√∫de do cear√° dr cabeto testa p...   \n",
       "4   nand√£o curtindo que o bolsonarismo est√° viran...   \n",
       "5                 queria o tema vai ser corona v√≠rus   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "1  desespero nos eua  chegou a obrigar por lei qu...   \n",
       "2   abestadohhh o presidente realmente estava doente   \n",
       "3  secretario da saude do ceara dr cabeto testa p...   \n",
       "4   nandao curtindo que o bolsonarismo esta viran...   \n",
       "5                 queria o tema vai ser corona virus   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "1  desespero nos eua  chegou a obrigar por lei qu...   \n",
       "2   abestadohhh o presidente realmente estava doente   \n",
       "3  secretario da saude do ceara dr cabeto testa p...   \n",
       "4   nandao curtindo que o bolsonarismo esta viran...   \n",
       "5                 queria o tema vai ser corona virus   \n",
       "\n",
       "                                               token  \\\n",
       "1  [serao, lei, panico, amigos, por, de, respirad...   \n",
       "2  [abestadohhh, realmente, presidente, doente, e...   \n",
       "3  [testa, saude, novo, coronavirus, cabeto, posi...   \n",
       "4  [minhas, isso, nandotenho, curtindo, falou, na...   \n",
       "5            [queria, tema, vai, corona, virus, ser]   \n",
       "\n",
       "                                              token2  \\\n",
       "1  ['serao', 'lei', 'panico', 'eua', 'obrigar', '...   \n",
       "2              ['realmente', 'doente', 'presidente']   \n",
       "3  ['saude', 'novo', 'coronavirus', 'cabeto', 'po...   \n",
       "4                 ['bolsonarismo', 'me', 'besteira']   \n",
       "5                                ['corona', 'virus']   \n",
       "\n",
       "                                         token3  \n",
       "1  [serao, lei, panico, eua, obrigar, fabricar]  \n",
       "2               [realmente, doente, presidente]  \n",
       "3           [saude, novo, cabeto, positivo, dr]  \n",
       "4                      [bolsonarismo, besteira]  \n",
       "5                                            []  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('2803_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0,5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "      <td>created_at</td>\n",
       "      <td>full_text</td>\n",
       "      <td>country_code</td>\n",
       "      <td>lang</td>\n",
       "      <td>latitude</td>\n",
       "      <td>longitude</td>\n",
       "      <td>location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243689256205406208</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>√â bom mostrar na tv mesmo o rosto e depoimento...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243689256163463169</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@Eli3nai_ poste, azar √© quarentena</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Catarina, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243689255534239744</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@ZemaVieira eu indo pro tecno depois da quaren...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1243689255471325185</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>quarentena por favor acabe vc est√° me matando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sorocaos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0                   id           created_at   \n",
       "1  1243689256205406208  2020-03-27 23:59:59   \n",
       "2  1243689256163463169  2020-03-27 23:59:59   \n",
       "3  1243689255534239744  2020-03-27 23:59:59   \n",
       "4  1243689255471325185  2020-03-27 23:59:59   \n",
       "\n",
       "                                           full_text  country_code  lang  \\\n",
       "0                                          full_text  country_code  lang   \n",
       "1  √â bom mostrar na tv mesmo o rosto e depoimento...           NaN    pt   \n",
       "2                 @Eli3nai_ poste, azar √© quarentena           NaN    pt   \n",
       "3  @ZemaVieira eu indo pro tecno depois da quaren...           NaN    pt   \n",
       "4      quarentena por favor acabe vc est√° me matando           NaN    pt   \n",
       "\n",
       "   latitude  longitude                location  \n",
       "0  latitude  longitude                location  \n",
       "1       NaN        NaN                      AM  \n",
       "2       NaN        NaN  Santa Catarina, Brasil  \n",
       "3       NaN        NaN                     NaN  \n",
       "4       NaN        NaN                sorocaos  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243689256205406208</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>√â bom mostrar na tv mesmo o rosto e depoimento...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243689256163463169</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@Eli3nai_ poste, azar √© quarentena</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santa Catarina, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243689255534239744</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@ZemaVieira eu indo pro tecno depois da quaren...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1243689255471325185</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>quarentena por favor acabe vc est√° me matando</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sorocaos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1243689254401826819</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@carlosmaxhado a m√£e de todo mundo t√° fumando ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campinas, Brasil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "1  1243689256205406208  2020-03-27 23:59:59   \n",
       "2  1243689256163463169  2020-03-27 23:59:59   \n",
       "3  1243689255534239744  2020-03-27 23:59:59   \n",
       "4  1243689255471325185  2020-03-27 23:59:59   \n",
       "5  1243689254401826819  2020-03-27 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "1  √â bom mostrar na tv mesmo o rosto e depoimento...          NaN   pt   \n",
       "2                 @Eli3nai_ poste, azar √© quarentena          NaN   pt   \n",
       "3  @ZemaVieira eu indo pro tecno depois da quaren...          NaN   pt   \n",
       "4      quarentena por favor acabe vc est√° me matando          NaN   pt   \n",
       "5  @carlosmaxhado a m√£e de todo mundo t√° fumando ...          NaN   pt   \n",
       "\n",
       "  latitude longitude                location  \n",
       "1      NaN       NaN                      AM  \n",
       "2      NaN       NaN  Santa Catarina, Brasil  \n",
       "3      NaN       NaN                     NaN  \n",
       "4      NaN       NaN                sorocaos  \n",
       "5      NaN       NaN        Campinas, Brasil  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243689256205406208</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>√â bom mostrar na tv mesmo o rosto e depoimento...</td>\n",
       "      <td>√© bom mostrar na tv mesmo o rosto e depoimento...</td>\n",
       "      <td>e bom mostrar na tv mesmo o rosto e depoimento...</td>\n",
       "      <td>e bom mostrar na tv mesmo o rosto e depoimento...</td>\n",
       "      <td>[imundo, depoimento, mesmo, de, chamar, na, do...</td>\n",
       "      <td>['chamar', 'bom', 'imundo', 'depoimento', 'tv'...</td>\n",
       "      <td>[chamar, bom, imundo, depoimento, tv, mostrar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243689256163463169</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@Eli3nai_ poste, azar √© quarentena</td>\n",
       "      <td>poste azar √© quarentena</td>\n",
       "      <td>poste azar e quarentena</td>\n",
       "      <td>poste azar e quarentena</td>\n",
       "      <td>[poste, azar, quarentena]</td>\n",
       "      <td>['azar']</td>\n",
       "      <td>[azar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243689255534239744</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@ZemaVieira eu indo pro tecno depois da quaren...</td>\n",
       "      <td>eu indo pro tecno depois da quarentena</td>\n",
       "      <td>eu indo pro tecno depois da quarentena</td>\n",
       "      <td>eu indo pro tecno depois da quarentena</td>\n",
       "      <td>[tecno, depois, pro, da, quarentena, eu, indo]</td>\n",
       "      <td>['depois']</td>\n",
       "      <td>[depois]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1243689255471325185</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>quarentena por favor acabe vc est√° me matando</td>\n",
       "      <td>quarentena por favor acabe vc est√° me matando</td>\n",
       "      <td>quarentena por favor acabe vc esta me matando</td>\n",
       "      <td>quarentena por favor acabe vc esta me matando</td>\n",
       "      <td>[matando, favor, me, acabe, por, quarentena, v...</td>\n",
       "      <td>['favor', 'me']</td>\n",
       "      <td>[favor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1243689254401826819</td>\n",
       "      <td>2020-03-27 23:59:59</td>\n",
       "      <td>@carlosmaxhado a m√£e de todo mundo t√° fumando ...</td>\n",
       "      <td>a m√£e de todo mundo t√° fumando maconha na qua...</td>\n",
       "      <td>a mae de todo mundo ta fumando maconha na qua...</td>\n",
       "      <td>a mae de todo mundo ta fumando maconha na qua...</td>\n",
       "      <td>[na, ta, sonho, mae, todo, fumando, quarentena...</td>\n",
       "      <td>['sonho', 'mae', 'todo', 'mundo', 'maconha']</td>\n",
       "      <td>[sonho, mae, mundo, maconha]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "1  1243689256205406208  2020-03-27 23:59:59   \n",
       "2  1243689256163463169  2020-03-27 23:59:59   \n",
       "3  1243689255534239744  2020-03-27 23:59:59   \n",
       "4  1243689255471325185  2020-03-27 23:59:59   \n",
       "5  1243689254401826819  2020-03-27 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "1  √â bom mostrar na tv mesmo o rosto e depoimento...   \n",
       "2                 @Eli3nai_ poste, azar √© quarentena   \n",
       "3  @ZemaVieira eu indo pro tecno depois da quaren...   \n",
       "4      quarentena por favor acabe vc est√° me matando   \n",
       "5  @carlosmaxhado a m√£e de todo mundo t√° fumando ...   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "1  √© bom mostrar na tv mesmo o rosto e depoimento...   \n",
       "2                            poste azar √© quarentena   \n",
       "3             eu indo pro tecno depois da quarentena   \n",
       "4      quarentena por favor acabe vc est√° me matando   \n",
       "5   a m√£e de todo mundo t√° fumando maconha na qua...   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "1  e bom mostrar na tv mesmo o rosto e depoimento...   \n",
       "2                            poste azar e quarentena   \n",
       "3             eu indo pro tecno depois da quarentena   \n",
       "4      quarentena por favor acabe vc esta me matando   \n",
       "5   a mae de todo mundo ta fumando maconha na qua...   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "1  e bom mostrar na tv mesmo o rosto e depoimento...   \n",
       "2                            poste azar e quarentena   \n",
       "3             eu indo pro tecno depois da quarentena   \n",
       "4      quarentena por favor acabe vc esta me matando   \n",
       "5   a mae de todo mundo ta fumando maconha na qua...   \n",
       "\n",
       "                                               token  \\\n",
       "1  [imundo, depoimento, mesmo, de, chamar, na, do...   \n",
       "2                          [poste, azar, quarentena]   \n",
       "3     [tecno, depois, pro, da, quarentena, eu, indo]   \n",
       "4  [matando, favor, me, acabe, por, quarentena, v...   \n",
       "5  [na, ta, sonho, mae, todo, fumando, quarentena...   \n",
       "\n",
       "                                              token2  \\\n",
       "1  ['chamar', 'bom', 'imundo', 'depoimento', 'tv'...   \n",
       "2                                           ['azar']   \n",
       "3                                         ['depois']   \n",
       "4                                    ['favor', 'me']   \n",
       "5       ['sonho', 'mae', 'todo', 'mundo', 'maconha']   \n",
       "\n",
       "                                              token3  \n",
       "1  [chamar, bom, imundo, depoimento, tv, mostrar,...  \n",
       "2                                             [azar]  \n",
       "3                                           [depois]  \n",
       "4                                            [favor]  \n",
       "5                       [sonho, mae, mundo, maconha]  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('2703_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_26.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1243326867702198275</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>@Juuh_sagawa vc ta fazendo oq o bolsonaro querüò†</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sorocaba, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243326867664494592</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>at√© o final dessa quarentena vou assistir goss...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guaraciaba do Norte, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243326866871681024</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>Nessa quarentena eu t√¥ contribuindo pra fuder ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Algarve, Portugal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243326866750046208</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>11¬∞ dia de quarentena ‚ù§üé∂ https://t.co/Rw5vfLRvt2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Londrina, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1243326865886138372</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>6¬∫ dia de quarentena: j√° comecei a experimenta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dourados, MS - Brasil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1243326867702198275  2020-03-26 23:59:59   \n",
       "1  1243326867664494592  2020-03-26 23:59:59   \n",
       "2  1243326866871681024  2020-03-26 23:59:59   \n",
       "3  1243326866750046208  2020-03-26 23:59:59   \n",
       "4  1243326865886138372  2020-03-26 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "0    @Juuh_sagawa vc ta fazendo oq o bolsonaro querüò†          NaN   pt   \n",
       "1  at√© o final dessa quarentena vou assistir goss...          NaN   pt   \n",
       "2  Nessa quarentena eu t√¥ contribuindo pra fuder ...          NaN   pt   \n",
       "3   11¬∞ dia de quarentena ‚ù§üé∂ https://t.co/Rw5vfLRvt2          NaN   pt   \n",
       "4  6¬∫ dia de quarentena: j√° comecei a experimenta...          NaN   pt   \n",
       "\n",
       "   latitude  longitude                     location  \n",
       "0       NaN        NaN             Sorocaba, Brasil  \n",
       "1       NaN        NaN  Guaraciaba do Norte, Brasil  \n",
       "2       NaN        NaN           Algarve, Portugal.  \n",
       "3       NaN        NaN             Londrina, Brasil  \n",
       "4       NaN        NaN        Dourados, MS - Brasil  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude', 'location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1243326867702198275</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>@Juuh_sagawa vc ta fazendo oq o bolsonaro querüò†</td>\n",
       "      <td>vc ta fazendo oq o bolsonaro querüò†</td>\n",
       "      <td>vc ta fazendo oq o bolsonaro quer</td>\n",
       "      <td>vc ta fazendo oq o bolsonaro quer</td>\n",
       "      <td>[ta, bolsonaro, oq, quer, fazendo, vc]</td>\n",
       "      <td>['bolsonaro', 'oq']</td>\n",
       "      <td>[bolsonaro]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1243326867664494592</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>at√© o final dessa quarentena vou assistir goss...</td>\n",
       "      <td>at√© o final dessa quarentena vou assistir goss...</td>\n",
       "      <td>ate o final dessa quarentena vou assistir goss...</td>\n",
       "      <td>ate o final dessa quarentena vou assistir goss...</td>\n",
       "      <td>[gossip, final, assistir, umas, dessa, quarent...</td>\n",
       "      <td>['crtz', 'assistir', 'final']</td>\n",
       "      <td>[crtz, assistir, final]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243326866871681024</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>Nessa quarentena eu t√¥ contribuindo pra fuder ...</td>\n",
       "      <td>nessa quarentena eu t√¥ contribuindo pra fuder ...</td>\n",
       "      <td>nessa quarentena eu to contribuindo pra fuder ...</td>\n",
       "      <td>nessa quarentena eu to contribuindo pra fuder ...</td>\n",
       "      <td>[to, quero, fuder, pessoas, serem, ver, os, eu...</td>\n",
       "      <td>['fuder', 'ver', 'likes', 'cabelo']</td>\n",
       "      <td>[fuder, likes, cabelo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1243326866750046208</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>11¬∞ dia de quarentena ‚ù§üé∂ https://t.co/Rw5vfLRvt2</td>\n",
       "      <td>11¬∞ dia de quarentena ‚ù§üé∂</td>\n",
       "      <td>11 dia de quarentena</td>\n",
       "      <td>11 dia de quarentena</td>\n",
       "      <td>[de, dia, quarentena]</td>\n",
       "      <td>['dia']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1243326865886138372</td>\n",
       "      <td>2020-03-26 23:59:59</td>\n",
       "      <td>6¬∫ dia de quarentena: j√° comecei a experimenta...</td>\n",
       "      <td>6¬∫ dia de quarentena j√° comecei a experimentar...</td>\n",
       "      <td>6o dia de quarentena ja comecei a experimentar...</td>\n",
       "      <td>6o dia de quarentena ja comecei a experimentar...</td>\n",
       "      <td>[6o, comecei, experimentar, receitas, opcoes, ...</td>\n",
       "      <td>['experimentar', 'comida', 'me', 'salsicha', '...</td>\n",
       "      <td>[experimentar, comida, salsicha]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1243326867702198275  2020-03-26 23:59:59   \n",
       "1  1243326867664494592  2020-03-26 23:59:59   \n",
       "2  1243326866871681024  2020-03-26 23:59:59   \n",
       "3  1243326866750046208  2020-03-26 23:59:59   \n",
       "4  1243326865886138372  2020-03-26 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "0    @Juuh_sagawa vc ta fazendo oq o bolsonaro querüò†   \n",
       "1  at√© o final dessa quarentena vou assistir goss...   \n",
       "2  Nessa quarentena eu t√¥ contribuindo pra fuder ...   \n",
       "3   11¬∞ dia de quarentena ‚ù§üé∂ https://t.co/Rw5vfLRvt2   \n",
       "4  6¬∫ dia de quarentena: j√° comecei a experimenta...   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0                 vc ta fazendo oq o bolsonaro querüò†   \n",
       "1  at√© o final dessa quarentena vou assistir goss...   \n",
       "2  nessa quarentena eu t√¥ contribuindo pra fuder ...   \n",
       "3                          11¬∞ dia de quarentena ‚ù§üé∂    \n",
       "4  6¬∫ dia de quarentena j√° comecei a experimentar...   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0                  vc ta fazendo oq o bolsonaro quer   \n",
       "1  ate o final dessa quarentena vou assistir goss...   \n",
       "2  nessa quarentena eu to contribuindo pra fuder ...   \n",
       "3                             11 dia de quarentena     \n",
       "4  6o dia de quarentena ja comecei a experimentar...   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0                  vc ta fazendo oq o bolsonaro quer   \n",
       "1  ate o final dessa quarentena vou assistir goss...   \n",
       "2  nessa quarentena eu to contribuindo pra fuder ...   \n",
       "3                             11 dia de quarentena     \n",
       "4  6o dia de quarentena ja comecei a experimentar...   \n",
       "\n",
       "                                               token  \\\n",
       "0             [ta, bolsonaro, oq, quer, fazendo, vc]   \n",
       "1  [gossip, final, assistir, umas, dessa, quarent...   \n",
       "2  [to, quero, fuder, pessoas, serem, ver, os, eu...   \n",
       "3                              [de, dia, quarentena]   \n",
       "4  [6o, comecei, experimentar, receitas, opcoes, ...   \n",
       "\n",
       "                                              token2  \\\n",
       "0                                ['bolsonaro', 'oq']   \n",
       "1                      ['crtz', 'assistir', 'final']   \n",
       "2                ['fuder', 'ver', 'likes', 'cabelo']   \n",
       "3                                            ['dia']   \n",
       "4  ['experimentar', 'comida', 'me', 'salsicha', '...   \n",
       "\n",
       "                             token3  \n",
       "0                       [bolsonaro]  \n",
       "1           [crtz, assistir, final]  \n",
       "2            [fuder, likes, cabelo]  \n",
       "3                                []  \n",
       "4  [experimentar, comida, salsicha]  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('2603_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('corona_tweets_24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1242602092688269312</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>@rfalcao13 O primeiro passo √© come√ßar o proces...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Itu - SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1242602092596023298</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>os cara t√£o criticando o cara que \"escreveu a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1242602092554051584</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>@yassmincarvalh Kkkkkkkk, irm√£ eu t√¥ at√© falan...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S√£o Lu√≠s, Brasil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1242602092474372096</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>fui ver o pronunciamento e EI BOLSONARO VAI TO...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gg stan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242602092470173699</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>O bolsonaro e toda sua fam√≠lia precisam urgent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z.O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1242602092688269312  2020-03-24 23:59:59   \n",
       "1  1242602092596023298  2020-03-24 23:59:59   \n",
       "2  1242602092554051584  2020-03-24 23:59:59   \n",
       "3  1242602092474372096  2020-03-24 23:59:59   \n",
       "4  1242602092470173699  2020-03-24 23:59:59   \n",
       "\n",
       "                                           full_text country_code lang  \\\n",
       "0  @rfalcao13 O primeiro passo √© come√ßar o proces...          NaN   pt   \n",
       "1  os cara t√£o criticando o cara que \"escreveu a ...          NaN   pt   \n",
       "2  @yassmincarvalh Kkkkkkkk, irm√£ eu t√¥ at√© falan...          NaN   pt   \n",
       "3  fui ver o pronunciamento e EI BOLSONARO VAI TO...          NaN   pt   \n",
       "4  O bolsonaro e toda sua fam√≠lia precisam urgent...          NaN   pt   \n",
       "\n",
       "   latitude  longitude          location  \n",
       "0       NaN        NaN          Itu - SP  \n",
       "1       NaN        NaN               NaN  \n",
       "2       NaN        NaN  S√£o Lu√≠s, Brasil  \n",
       "3       NaN        NaN           gg stan  \n",
       "4       NaN        NaN               Z.O  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang', 'latitude', 'longitude','location'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1242602092688269312</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>@rfalcao13 O primeiro passo √© come√ßar o proces...</td>\n",
       "      <td>o primeiro passo √© come√ßar o processo para ti...</td>\n",
       "      <td>o primeiro passo e comecar o processo para ti...</td>\n",
       "      <td>o primeiro passo e comecar o processo para ti...</td>\n",
       "      <td>[mas, processo, apoiam, insano, politicos, emp...</td>\n",
       "      <td>['processo', 'insano', 'politicos', 'empresari...</td>\n",
       "      <td>[processo, insano, politicos, empresarios, pov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1242602092596023298</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>os cara t√£o criticando o cara que \"escreveu a ...</td>\n",
       "      <td>os cara t√£o criticando o cara que \"escreveu a ...</td>\n",
       "      <td>os cara tao criticando o cara que \"escreveu a ...</td>\n",
       "      <td>os cara tao criticando o cara que \"escreveu a ...</td>\n",
       "      <td>[cego, escreveu, muito, os, esse, que, verme, ...</td>\n",
       "      <td>['muito', 'tao', 'verme', 'bolsonaro', 'cego']</td>\n",
       "      <td>[muito, verme, bolsonaro, cego]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1242602092554051584</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>@yassmincarvalh Kkkkkkkk, irm√£ eu t√¥ at√© falan...</td>\n",
       "      <td>kkkkkkkk irm√£ eu t√¥ at√© falando coisas p @ qu...</td>\n",
       "      <td>kkkkkkkk irma eu to ate falando coisas p @ qu...</td>\n",
       "      <td>kkkkkkkk irma eu to ate falando coisas p @ qu...</td>\n",
       "      <td>[falando, irma, to, confiando, diria, kkkkkkkk...</td>\n",
       "      <td>['irma', 'so', 'jamais', 'kkkkkkkk']</td>\n",
       "      <td>[irma, jamais]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1242602092474372096</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>fui ver o pronunciamento e EI BOLSONARO VAI TO...</td>\n",
       "      <td>fui ver o pronunciamento e ei bolsonaro vai to...</td>\n",
       "      <td>fui ver o pronunciamento e ei bolsonaro vai to...</td>\n",
       "      <td>fui ver o pronunciamento e ei bolsonaro vai to...</td>\n",
       "      <td>[fui, tomar, ver, vai, pronunciamento, bolsona...</td>\n",
       "      <td>['tomar', 'ver', 'pronunciamento', 'bolsonaro'...</td>\n",
       "      <td>[tomar, pronunciamento, bolsonaro, cu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1242602092470173699</td>\n",
       "      <td>2020-03-24 23:59:59</td>\n",
       "      <td>O bolsonaro e toda sua fam√≠lia precisam urgent...</td>\n",
       "      <td>o bolsonaro e toda sua fam√≠lia precisam urgent...</td>\n",
       "      <td>o bolsonaro e toda sua familia precisam urgent...</td>\n",
       "      <td>o bolsonaro e toda sua familia precisam urgent...</td>\n",
       "      <td>[canalhice, um, onde, precisam, sua, senso, va...</td>\n",
       "      <td>['senso', 'pouco', 'ridiculo', 'bolsonaro', 'f...</td>\n",
       "      <td>[senso, pouco, ridiculo, bolsonaro, familia, u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1242602092688269312  2020-03-24 23:59:59   \n",
       "1  1242602092596023298  2020-03-24 23:59:59   \n",
       "2  1242602092554051584  2020-03-24 23:59:59   \n",
       "3  1242602092474372096  2020-03-24 23:59:59   \n",
       "4  1242602092470173699  2020-03-24 23:59:59   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  @rfalcao13 O primeiro passo √© come√ßar o proces...   \n",
       "1  os cara t√£o criticando o cara que \"escreveu a ...   \n",
       "2  @yassmincarvalh Kkkkkkkk, irm√£ eu t√¥ at√© falan...   \n",
       "3  fui ver o pronunciamento e EI BOLSONARO VAI TO...   \n",
       "4  O bolsonaro e toda sua fam√≠lia precisam urgent...   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0   o primeiro passo √© come√ßar o processo para ti...   \n",
       "1  os cara t√£o criticando o cara que \"escreveu a ...   \n",
       "2   kkkkkkkk irm√£ eu t√¥ at√© falando coisas p @ qu...   \n",
       "3  fui ver o pronunciamento e ei bolsonaro vai to...   \n",
       "4  o bolsonaro e toda sua fam√≠lia precisam urgent...   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0   o primeiro passo e comecar o processo para ti...   \n",
       "1  os cara tao criticando o cara que \"escreveu a ...   \n",
       "2   kkkkkkkk irma eu to ate falando coisas p @ qu...   \n",
       "3  fui ver o pronunciamento e ei bolsonaro vai to...   \n",
       "4  o bolsonaro e toda sua familia precisam urgent...   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0   o primeiro passo e comecar o processo para ti...   \n",
       "1  os cara tao criticando o cara que \"escreveu a ...   \n",
       "2   kkkkkkkk irma eu to ate falando coisas p @ qu...   \n",
       "3  fui ver o pronunciamento e ei bolsonaro vai to...   \n",
       "4  o bolsonaro e toda sua familia precisam urgent...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [mas, processo, apoiam, insano, politicos, emp...   \n",
       "1  [cego, escreveu, muito, os, esse, que, verme, ...   \n",
       "2  [falando, irma, to, confiando, diria, kkkkkkkk...   \n",
       "3  [fui, tomar, ver, vai, pronunciamento, bolsona...   \n",
       "4  [canalhice, um, onde, precisam, sua, senso, va...   \n",
       "\n",
       "                                              token2  \\\n",
       "0  ['processo', 'insano', 'politicos', 'empresari...   \n",
       "1     ['muito', 'tao', 'verme', 'bolsonaro', 'cego']   \n",
       "2               ['irma', 'so', 'jamais', 'kkkkkkkk']   \n",
       "3  ['tomar', 'ver', 'pronunciamento', 'bolsonaro'...   \n",
       "4  ['senso', 'pouco', 'ridiculo', 'bolsonaro', 'f...   \n",
       "\n",
       "                                              token3  \n",
       "0  [processo, insano, politicos, empresarios, pov...  \n",
       "1                    [muito, verme, bolsonaro, cego]  \n",
       "2                                     [irma, jamais]  \n",
       "3             [tomar, pronunciamento, bolsonaro, cu]  \n",
       "4  [senso, pouco, ridiculo, bolsonaro, familia, u...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('2403_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('tweets_0319_c_bolsonaro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240790152777281536</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>@Paivamanu_ kkkkkkkkkkkkkkkk aproveitar a quar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240790152613593090</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>@MttzinhuC @uiltoo2 negros de verdade t√£o com ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240790152462700549</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>@Atanasio_real @l03n27 o loen falando como iss...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240790152458506244</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>a quarentena ta causando s√©rios problemas ment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240790152454311939</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>Vou passar a quarentena com o meu am√¥ üò©‚úåüèªüòâü•∞</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id  created_at  \\\n",
       "0  1240790152777281536  2020-03-19   \n",
       "1  1240790152613593090  2020-03-19   \n",
       "2  1240790152462700549  2020-03-19   \n",
       "3  1240790152458506244  2020-03-19   \n",
       "4  1240790152454311939  2020-03-19   \n",
       "\n",
       "                                                text country_code lang  \n",
       "0  @Paivamanu_ kkkkkkkkkkkkkkkk aproveitar a quar...          NaN   pt  \n",
       "1  @MttzinhuC @uiltoo2 negros de verdade t√£o com ...          NaN   pt  \n",
       "2  @Atanasio_real @l03n27 o loen falando como iss...          NaN   pt  \n",
       "3  a quarentena ta causando s√©rios problemas ment...          NaN   pt  \n",
       "4        Vou passar a quarentena com o meu am√¥ üò©‚úåüèªüòâü•∞          NaN   pt  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240790152777281536</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>@Paivamanu_ kkkkkkkkkkkkkkkk aproveitar a quar...</td>\n",
       "      <td>kkkkkkkkkkkkkkkk aproveitar a quarentena p re...</td>\n",
       "      <td>kkkkkkkkkkkkkkkk aproveitar a quarentena p re...</td>\n",
       "      <td>kkkkkkkkkkkkkkkk aproveitar a quarentena p re...</td>\n",
       "      <td>[keru, kkkkkkkkkkkkkkkk, num, ficar, naum, apr...</td>\n",
       "      <td>['rezar', 'aproveitar', 'num', 'ficar']</td>\n",
       "      <td>[rezar, aproveitar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240790152613593090</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>@MttzinhuC @uiltoo2 negros de verdade t√£o com ...</td>\n",
       "      <td>negros de verdade t√£o com bolsonaro mano\\n\\n...</td>\n",
       "      <td>negros de verdade tao com bolsonaro mano\\n\\n...</td>\n",
       "      <td>negros de verdade tao com bolsonaro mano\\n\\n...</td>\n",
       "      <td>[negros, com, boy, esquerda, bolsonaro, verdad...</td>\n",
       "      <td>['boy', 'bolsonaro', 'verdade', 'branco', 'man...</td>\n",
       "      <td>[boy, bolsonaro, verdade, branco]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240790152462700549</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>@Atanasio_real @l03n27 o loen falando como iss...</td>\n",
       "      <td>o loen falando como isso √© coisa de novo-ric...</td>\n",
       "      <td>o loen falando como isso e coisa de novo-ric...</td>\n",
       "      <td>o loen falando como isso e coisa de novo-ric...</td>\n",
       "      <td>[falando, loen, mosca, na, coisa, novo-rico-de...</td>\n",
       "      <td>['coisa', 'corona']</td>\n",
       "      <td>[coisa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240790152458506244</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>a quarentena ta causando s√©rios problemas ment...</td>\n",
       "      <td>a quarentena ta causando s√©rios problemas ment...</td>\n",
       "      <td>a quarentena ta causando serios problemas ment...</td>\n",
       "      <td>a quarentena ta causando serios problemas ment...</td>\n",
       "      <td>[aos, ta, causando, problemas, serios, brasile...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240790152454311939</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>Vou passar a quarentena com o meu am√¥ üò©‚úåüèªüòâü•∞</td>\n",
       "      <td>vou passar a quarentena com o meu am√¥ üò©‚úåüèªüòâü•∞</td>\n",
       "      <td>vou passar a quarentena com o meu amo</td>\n",
       "      <td>vou passar a quarentena com o meu amo</td>\n",
       "      <td>[meu, passar, com, amo, quarentena, vou]</td>\n",
       "      <td>['meu', 'passar', 'amo']</td>\n",
       "      <td>[passar, amo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id  created_at  \\\n",
       "0  1240790152777281536  2020-03-19   \n",
       "1  1240790152613593090  2020-03-19   \n",
       "2  1240790152462700549  2020-03-19   \n",
       "3  1240790152458506244  2020-03-19   \n",
       "4  1240790152454311939  2020-03-19   \n",
       "\n",
       "                                                text  \\\n",
       "0  @Paivamanu_ kkkkkkkkkkkkkkkk aproveitar a quar...   \n",
       "1  @MttzinhuC @uiltoo2 negros de verdade t√£o com ...   \n",
       "2  @Atanasio_real @l03n27 o loen falando como iss...   \n",
       "3  a quarentena ta causando s√©rios problemas ment...   \n",
       "4        Vou passar a quarentena com o meu am√¥ üò©‚úåüèªüòâü•∞   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0   kkkkkkkkkkkkkkkk aproveitar a quarentena p re...   \n",
       "1    negros de verdade t√£o com bolsonaro mano\\n\\n...   \n",
       "2    o loen falando como isso √© coisa de novo-ric...   \n",
       "3  a quarentena ta causando s√©rios problemas ment...   \n",
       "4        vou passar a quarentena com o meu am√¥ üò©‚úåüèªüòâü•∞   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0   kkkkkkkkkkkkkkkk aproveitar a quarentena p re...   \n",
       "1    negros de verdade tao com bolsonaro mano\\n\\n...   \n",
       "2    o loen falando como isso e coisa de novo-ric...   \n",
       "3  a quarentena ta causando serios problemas ment...   \n",
       "4             vou passar a quarentena com o meu amo    \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0   kkkkkkkkkkkkkkkk aproveitar a quarentena p re...   \n",
       "1    negros de verdade tao com bolsonaro mano\\n\\n...   \n",
       "2    o loen falando como isso e coisa de novo-ric...   \n",
       "3  a quarentena ta causando serios problemas ment...   \n",
       "4             vou passar a quarentena com o meu amo    \n",
       "\n",
       "                                               token  \\\n",
       "0  [keru, kkkkkkkkkkkkkkkk, num, ficar, naum, apr...   \n",
       "1  [negros, com, boy, esquerda, bolsonaro, verdad...   \n",
       "2  [falando, loen, mosca, na, coisa, novo-rico-de...   \n",
       "3  [aos, ta, causando, problemas, serios, brasile...   \n",
       "4           [meu, passar, com, amo, quarentena, vou]   \n",
       "\n",
       "                                              token2  \\\n",
       "0            ['rezar', 'aproveitar', 'num', 'ficar']   \n",
       "1  ['boy', 'bolsonaro', 'verdade', 'branco', 'man...   \n",
       "2                                ['coisa', 'corona']   \n",
       "3                                                 []   \n",
       "4                           ['meu', 'passar', 'amo']   \n",
       "\n",
       "                              token3  \n",
       "0                [rezar, aproveitar]  \n",
       "1  [boy, bolsonaro, verdade, branco]  \n",
       "2                            [coisa]  \n",
       "3                                 []  \n",
       "4                      [passar, amo]  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('1903_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('tweets_0318_c_bolsonaro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240427765431746560</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>Terceiro dia de quarentena e nada mudou. j√° fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240427765326897152</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>@seonghwanlol This √© an manifesta√ß√£o against o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240427765192699907</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>Fam√≠lia, amigos, adoradores, estou bem!! N√£o e...</td>\n",
       "      <td>BR</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240427765062676481</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>quarentena n √© nenhum desafio pra mim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240427764664107009</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>@radaronline O pior √© ver o ministro da sa√∫de ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id  created_at  \\\n",
       "0  1240427765431746560  2020-03-18   \n",
       "1  1240427765326897152  2020-03-18   \n",
       "2  1240427765192699907  2020-03-18   \n",
       "3  1240427765062676481  2020-03-18   \n",
       "4  1240427764664107009  2020-03-18   \n",
       "\n",
       "                                                text country_code lang  \n",
       "0  Terceiro dia de quarentena e nada mudou. j√° fi...          NaN   pt  \n",
       "1  @seonghwanlol This √© an manifesta√ß√£o against o...          NaN   pt  \n",
       "2  Fam√≠lia, amigos, adoradores, estou bem!! N√£o e...           BR   pt  \n",
       "3              quarentena n √© nenhum desafio pra mim          NaN   pt  \n",
       "4  @radaronline O pior √© ver o ministro da sa√∫de ...          NaN   pt  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240427765431746560</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>Terceiro dia de quarentena e nada mudou. j√° fi...</td>\n",
       "      <td>terceiro dia de quarentena e nada mudou j√° fic...</td>\n",
       "      <td>terceiro dia de quarentena e nada mudou ja fic...</td>\n",
       "      <td>terceiro dia de quarentena e nada mudou ja fic...</td>\n",
       "      <td>[mudou, to, terceiro, fico, nada, em, sem, aco...</td>\n",
       "      <td>['terceiro', 'nada', 'todo', 'ja', 'casa', 'dia']</td>\n",
       "      <td>[terceiro, nada, casa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240427765326897152</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>@seonghwanlol This √© an manifesta√ß√£o against o...</td>\n",
       "      <td>this √© an manifesta√ß√£o against o president bo...</td>\n",
       "      <td>this e an manifestacao against o president bo...</td>\n",
       "      <td>this e an manifestacao against o president bo...</td>\n",
       "      <td>[uma, bolsonaro, day, had, of, this, support, ...</td>\n",
       "      <td>['bolsonaro', 'this']</td>\n",
       "      <td>[bolsonaro, this]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240427765192699907</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>Fam√≠lia, amigos, adoradores, estou bem!! N√£o e...</td>\n",
       "      <td>fam√≠lia amigos adoradores estou bem!! n√£o esto...</td>\n",
       "      <td>familia amigos adoradores estou bem!! nao esto...</td>\n",
       "      <td>familia amigos adoradores estou bem!! nao esto...</td>\n",
       "      <td>[estou, em, contrai, coronavirus, bem, com, ad...</td>\n",
       "      <td>['coronavirus', 'bem', 'familia', 'casa', 'so'...</td>\n",
       "      <td>[familia, casa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240427765062676481</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>quarentena n √© nenhum desafio pra mim</td>\n",
       "      <td>quarentena n √© nenhum desafio pra mim</td>\n",
       "      <td>quarentena n e nenhum desafio pra mim</td>\n",
       "      <td>quarentena n e nenhum desafio pra mim</td>\n",
       "      <td>[nenhum, desafio, mim, pra, quarentena]</td>\n",
       "      <td>['nenhum', 'desafio', 'mim']</td>\n",
       "      <td>[nenhum, desafio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240427764664107009</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>@radaronline O pior √© ver o ministro da sa√∫de ...</td>\n",
       "      <td>o pior √© ver o ministro da sa√∫de at√© ent√£o ma...</td>\n",
       "      <td>o pior e ver o ministro da saude ate entao ma...</td>\n",
       "      <td>o pior e ver o ministro da saude ate entao ma...</td>\n",
       "      <td>[infantil, saude, ver, da, de, pedido, na, ten...</td>\n",
       "      <td>['infantil', 'entao', 'saude', 'ver', 'compete...</td>\n",
       "      <td>[infantil, saude, competente, coletiva, minist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id  created_at  \\\n",
       "0  1240427765431746560  2020-03-18   \n",
       "1  1240427765326897152  2020-03-18   \n",
       "2  1240427765192699907  2020-03-18   \n",
       "3  1240427765062676481  2020-03-18   \n",
       "4  1240427764664107009  2020-03-18   \n",
       "\n",
       "                                                text  \\\n",
       "0  Terceiro dia de quarentena e nada mudou. j√° fi...   \n",
       "1  @seonghwanlol This √© an manifesta√ß√£o against o...   \n",
       "2  Fam√≠lia, amigos, adoradores, estou bem!! N√£o e...   \n",
       "3              quarentena n √© nenhum desafio pra mim   \n",
       "4  @radaronline O pior √© ver o ministro da sa√∫de ...   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0  terceiro dia de quarentena e nada mudou j√° fic...   \n",
       "1   this √© an manifesta√ß√£o against o president bo...   \n",
       "2  fam√≠lia amigos adoradores estou bem!! n√£o esto...   \n",
       "3              quarentena n √© nenhum desafio pra mim   \n",
       "4   o pior √© ver o ministro da sa√∫de at√© ent√£o ma...   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0  terceiro dia de quarentena e nada mudou ja fic...   \n",
       "1   this e an manifestacao against o president bo...   \n",
       "2  familia amigos adoradores estou bem!! nao esto...   \n",
       "3              quarentena n e nenhum desafio pra mim   \n",
       "4   o pior e ver o ministro da saude ate entao ma...   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0  terceiro dia de quarentena e nada mudou ja fic...   \n",
       "1   this e an manifestacao against o president bo...   \n",
       "2  familia amigos adoradores estou bem!! nao esto...   \n",
       "3              quarentena n e nenhum desafio pra mim   \n",
       "4   o pior e ver o ministro da saude ate entao ma...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [mudou, to, terceiro, fico, nada, em, sem, aco...   \n",
       "1  [uma, bolsonaro, day, had, of, this, support, ...   \n",
       "2  [estou, em, contrai, coronavirus, bem, com, ad...   \n",
       "3            [nenhum, desafio, mim, pra, quarentena]   \n",
       "4  [infantil, saude, ver, da, de, pedido, na, ten...   \n",
       "\n",
       "                                              token2  \\\n",
       "0  ['terceiro', 'nada', 'todo', 'ja', 'casa', 'dia']   \n",
       "1                              ['bolsonaro', 'this']   \n",
       "2  ['coronavirus', 'bem', 'familia', 'casa', 'so'...   \n",
       "3                       ['nenhum', 'desafio', 'mim']   \n",
       "4  ['infantil', 'entao', 'saude', 'ver', 'compete...   \n",
       "\n",
       "                                              token3  \n",
       "0                             [terceiro, nada, casa]  \n",
       "1                                  [bolsonaro, this]  \n",
       "2                                    [familia, casa]  \n",
       "3                                  [nenhum, desafio]  \n",
       "4  [infantil, saude, competente, coletiva, minist...  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('1803_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17/03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input do dataframe de 01/04\n",
    "\n",
    "df = pd.read_csv('tweets_0317_c_bolsonaro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>country_code</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240065377520037888</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>O problema de estar falando toda hora da doen√ß...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240065376739889153</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>esse bgl de corona v√≠rus t√° chat√£o j√°, acaband...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240065376580485120</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>Ou lix√£o o que vc fez at√© agora?al√©m de perseg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240065376458887173</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>@carols_lacerda esse corona virus foi longe de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240065376068780033</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>kkkkkk o cara do corre ta com corona</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id  created_at  \\\n",
       "0  1240065377520037888  2020-03-17   \n",
       "1  1240065376739889153  2020-03-17   \n",
       "2  1240065376580485120  2020-03-17   \n",
       "3  1240065376458887173  2020-03-17   \n",
       "4  1240065376068780033  2020-03-17   \n",
       "\n",
       "                                                text country_code lang  \n",
       "0  O problema de estar falando toda hora da doen√ß...          NaN   pt  \n",
       "1  esse bgl de corona v√≠rus t√° chat√£o j√°, acaband...          NaN   pt  \n",
       "2  Ou lix√£o o que vc fez at√© agora?al√©m de perseg...          NaN   pt  \n",
       "3  @carols_lacerda esse corona virus foi longe de...          NaN   pt  \n",
       "4               kkkkkk o cara do corre ta com corona          NaN   pt  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppando colunas irrelevantes\n",
    "\n",
    "df = df.drop(['country_code', 'lang'], axis = 1)\n",
    "\n",
    "# criando listas para incluir os valores\n",
    "letras = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    letras.append(strip_characters(df.iloc[row,2]))\n",
    "    \n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_letras'] = letras\n",
    "\n",
    "# remove acentua√ß√£o de todos os tokens\n",
    "if __name__ == '__main__':\n",
    "        from doctest import testmod\n",
    "        testmod()\n",
    "\n",
    "# criando listas para incluir os valores        \n",
    "acentos = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    acentos.append(remover_acentos(df.iloc[row,3]))\n",
    "\n",
    "# inclus√£o dos dados limpos no datafame\n",
    "df['txt_acentos'] = acentos\n",
    "\n",
    "# criando listas para incluir os valores sem emoji\n",
    "token_emoji_free = []\n",
    "\n",
    "# loop para garantir que todas as linhas do dataframe sejam analisadas\n",
    "for row in range(0, len(df)):\n",
    "    token_emoji_free.append(strip_emoji(df.iloc[row,4]))\n",
    "        \n",
    "# inclus√£o dos dados limpos de emoji no datafame\n",
    "df['txt_emoji_free'] = token_emoji_free\n",
    "\n",
    "# transformando tudo em tokens\n",
    "df['token'] = df['txt_emoji_free'].apply(lambda x: tokenize(x))\n",
    "\n",
    "# filtranso os tokens para aparecer apenas os 10 mil mais frequentes\n",
    "df['token2'] = df.token.apply(lambda x: str(list(set(eval(str(x))).intersection(lista_token))))\n",
    "\n",
    "# transforma os tokens em lista\n",
    "lista = list(df['token2'])\n",
    "\n",
    "# Criando Bigram e Trigram\n",
    "\n",
    "bigram = gensim.models.Phrases(lista, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram[lista], threshold=100)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# Processando os tweets\n",
    "texto = process_words(lista)\n",
    "\n",
    "# inserindo no df\n",
    "df['token3'] = texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>txt_letras</th>\n",
       "      <th>txt_acentos</th>\n",
       "      <th>txt_emoji_free</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>token3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240065377520037888</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>O problema de estar falando toda hora da doen√ß...</td>\n",
       "      <td>o problema de estar falando toda hora da doen√ß...</td>\n",
       "      <td>o problema de estar falando toda hora da doenc...</td>\n",
       "      <td>o problema de estar falando toda hora da doenc...</td>\n",
       "      <td>[falando, hora, da, ja, de, problema, com, gen...</td>\n",
       "      <td>['ja', 'hora', 'diferente', 'corona', 'gente',...</td>\n",
       "      <td>[hora, diferente, gente, problema]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1240065376739889153</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>esse bgl de corona v√≠rus t√° chat√£o j√°, acaband...</td>\n",
       "      <td>esse bgl de corona v√≠rus t√° chat√£o j√° acabando...</td>\n",
       "      <td>esse bgl de corona virus ta chatao ja acabando...</td>\n",
       "      <td>esse bgl de corona virus ta chatao ja acabando...</td>\n",
       "      <td>[ja, ta, esse, com, corona, chatao, bgl, meus,...</td>\n",
       "      <td>['ja', 'corona', 'virus']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1240065376580485120</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>Ou lix√£o o que vc fez at√© agora?al√©m de perseg...</td>\n",
       "      <td>ou lix√£o o que vc fez at√© agoraal√©m de persegu...</td>\n",
       "      <td>ou lixao o que vc fez ate agoraalem de persegu...</td>\n",
       "      <td>ou lixao o que vc fez ate agoraalem de persegu...</td>\n",
       "      <td>[marido, lixo, cambada, ja, de, assim, vao, co...</td>\n",
       "      <td>['vao', 'gordo', 'marido', 'perseguir', 'vagab...</td>\n",
       "      <td>[gordo, marido, perseguir, vagabundo, presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1240065376458887173</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>@carols_lacerda esse corona virus foi longe de...</td>\n",
       "      <td>esse corona virus foi longe demais agora</td>\n",
       "      <td>esse corona virus foi longe demais agora</td>\n",
       "      <td>esse corona virus foi longe demais agora</td>\n",
       "      <td>[demais, longe, esse, agora, corona, virus, foi]</td>\n",
       "      <td>['demais', 'longe', 'agora', 'corona', 'virus']</td>\n",
       "      <td>[demais, longe, agora]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240065376068780033</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>kkkkkk o cara do corre ta com corona</td>\n",
       "      <td>kkkkkk o cara do corre ta com corona</td>\n",
       "      <td>kkkkkk o cara do corre ta com corona</td>\n",
       "      <td>kkkkkk o cara do corre ta com corona</td>\n",
       "      <td>[ta, kkkkkk, com, corona, do, cara, corre]</td>\n",
       "      <td>['kkkkkk', 'corona']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             status_id  created_at  \\\n",
       "0  1240065377520037888  2020-03-17   \n",
       "1  1240065376739889153  2020-03-17   \n",
       "2  1240065376580485120  2020-03-17   \n",
       "3  1240065376458887173  2020-03-17   \n",
       "4  1240065376068780033  2020-03-17   \n",
       "\n",
       "                                                text  \\\n",
       "0  O problema de estar falando toda hora da doen√ß...   \n",
       "1  esse bgl de corona v√≠rus t√° chat√£o j√°, acaband...   \n",
       "2  Ou lix√£o o que vc fez at√© agora?al√©m de perseg...   \n",
       "3  @carols_lacerda esse corona virus foi longe de...   \n",
       "4               kkkkkk o cara do corre ta com corona   \n",
       "\n",
       "                                          txt_letras  \\\n",
       "0  o problema de estar falando toda hora da doen√ß...   \n",
       "1  esse bgl de corona v√≠rus t√° chat√£o j√° acabando...   \n",
       "2  ou lix√£o o que vc fez at√© agoraal√©m de persegu...   \n",
       "3           esse corona virus foi longe demais agora   \n",
       "4               kkkkkk o cara do corre ta com corona   \n",
       "\n",
       "                                         txt_acentos  \\\n",
       "0  o problema de estar falando toda hora da doenc...   \n",
       "1  esse bgl de corona virus ta chatao ja acabando...   \n",
       "2  ou lixao o que vc fez ate agoraalem de persegu...   \n",
       "3           esse corona virus foi longe demais agora   \n",
       "4               kkkkkk o cara do corre ta com corona   \n",
       "\n",
       "                                      txt_emoji_free  \\\n",
       "0  o problema de estar falando toda hora da doenc...   \n",
       "1  esse bgl de corona virus ta chatao ja acabando...   \n",
       "2  ou lixao o que vc fez ate agoraalem de persegu...   \n",
       "3           esse corona virus foi longe demais agora   \n",
       "4               kkkkkk o cara do corre ta com corona   \n",
       "\n",
       "                                               token  \\\n",
       "0  [falando, hora, da, ja, de, problema, com, gen...   \n",
       "1  [ja, ta, esse, com, corona, chatao, bgl, meus,...   \n",
       "2  [marido, lixo, cambada, ja, de, assim, vao, co...   \n",
       "3   [demais, longe, esse, agora, corona, virus, foi]   \n",
       "4         [ta, kkkkkk, com, corona, do, cara, corre]   \n",
       "\n",
       "                                              token2  \\\n",
       "0  ['ja', 'hora', 'diferente', 'corona', 'gente',...   \n",
       "1                          ['ja', 'corona', 'virus']   \n",
       "2  ['vao', 'gordo', 'marido', 'perseguir', 'vagab...   \n",
       "3    ['demais', 'longe', 'agora', 'corona', 'virus']   \n",
       "4                               ['kkkkkk', 'corona']   \n",
       "\n",
       "                                              token3  \n",
       "0                 [hora, diferente, gente, problema]  \n",
       "1                                                 []  \n",
       "2  [gordo, marido, perseguir, vagabundo, presiden...  \n",
       "3                             [demais, longe, agora]  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportar o DF\n",
    "df.to_csv('1703_preprocessado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
